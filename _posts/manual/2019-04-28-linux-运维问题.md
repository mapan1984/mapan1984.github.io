---
title: Linux 运维问题
tags: [Linux]
---

### Inode 节点占满

查看磁盘占用：

    $ df -i
    Filesystem            Inodes   IUsed   IFree IUse% Mounted on
    /dev/vda1            1310720 1310720       0  100% /
    tmpfs                 983191       1  983190    1% /dev/shm
    /dev/vdb             13107200   91566 13015634    1% /data

    $ df -h
    Filesystem            Size  Used Avail Use% Mounted on
    /dev/vda1              20G  9.0G  9.8G  48% /
    tmpfs                 3.8G     0  3.8G   0% /dev/shm
    /dev/vdb              197G  131G   57G  71% /data

找出文件数量最多的目录：

    $ for i in /*; do echo $i; find $i | wc -l; done

删除文件：

    $ find dir -type f -name '*'  | xargs rm

    $ find /var/spool/clientmqueue -type f -mtime +30 -exec rm -f {} \;

### 进程跑满

错误：

    -bash: fork: retry: No child processes

查看进程数限制：

    $ cat /proc/sys/kernel/pid_max
    49152

调整限制：

    $ echo 100000 > /proc/sys/kernel/pid_max

或者查看是否有无用的进程：

    $ pstree -up

### 删除文件未释放磁盘占用

    $ lsof | grep delete

    $ find /proc/*/fd -ls | grep  '(deleted)'
    $ : > "/proc/$pid/fd/$fd"


``` sh
#!/bin/bash

delete_files=$(find /proc/*/fd -ls 2>/dev/null | grep  '(deleted)' | awk '{print $11}')
for delete_file in ${delete_files}; do
    echo "echom "" > ${delete_file}"
    : > ${delete_file}
done
```

### 清除 cache

仅清除页面缓存（PageCache）

    $ sync; echo 1 > /proc/sys/vm/drop_caches

清除目录项和inode

    $ sync; echo 2 > /proc/sys/vm/drop_caches

清除页面缓存，目录项和inode

    $ sync; echo 3 > /proc/sys/vm/drop_caches

当内存小于 90G 时自动清理：

``` bash
#!/bin/bash

#
# /etc/cron.hourly/drop_caches.sh
#

free_mem=$(free -g | awk '/^Mem/ {print $4}')
echo "$(date +"%Y-%m-%d-%T") - before free_mem: $free_mem" >> /root/drop_caches.log 2>&1

if [[ free_mem -lt 90 ]]; then
  echo "$(date +"%Y-%m-%d-%T") - drop caches" >> /root/drop_caches.log 2>&1
  sync; echo 1 > /proc/sys/vm/drop_caches
else
  echo "$(date +"%Y-%m-%d-%T") - do  nothing" >> /root/drop_caches.log 2>&1
fi

free_mem=$(free -g | awk '/^Mem/ {print $4}')
echo "$(date +"%Y-%m-%d-%T") -  after free_mem: $free_mem" >> /root/drop_caches.log 2>&1
```

### messages

    $ cat /var/log/messages
    ...
    kernel: SLUB: Unable to allocate memory on node -1 (gfp=0x8020)
    ...

### 连接过多丢包

`/var/log/messages`

    Jan 17 11:12:09 xxg-udw38 kernel: [107394376.115531] nf_conntrack: table full, dropping packet.

`/etc/sysctl.conf`

If the problem is caused by a large number of connections, try to increase the value of `net.ipv4.netfilter.ip_conntrack_max`:

``` sh
sysctl -w net.ip4.netfilter.ip_conntrack_max=655360 //RHEL 5
sysctl -w net.netfilter.nf_conntrack_max=655360 //RHEL 6+
```

To make the changes permanent, add the following in `/etc/sysctl.conf`:

``` sh
net.ip4.netfilter.ip_conntrack_max=655360 //RHEL 5
net.netfilter.nf_conntrack_max=655360 //RHEL 6+
```

As a general best practice, do not allow too many connections on a server. A conntrack consumes 400 bytes in the kernel (see /proc/slabinfo). This means tracking 655360 connections would consume 262MB of RAM.


``` sh
for i in `docker ps | awk 'NR>1 {print $NF}'`; do
    echo $i;
    docker exec $i cat /proc/sys/net/netfilter/nf_conntrack_count;
done
```

### overcommit_memory

* vm.overcommit_memory=0：只要有空闲的物理内存，就允许给进程分配
* vm.overcommit_memory=1：内核在分配的时候不做检查，进程真正使用的时候，在pagefault里可能会失败
* vm.overcommit_memory=2：允许分配超过所有物理内存和交换空间总和的内存，上限是 swap + 物理mem * ratio
    * vm.overcommit_memory=2，vm.overcommit_ratio=90


查看参数：

    $ sysctl vm.overcommit_memory

修改参数值：

    $ sysctl -w vm.overcommit_ratio=90

文件：`/etc/sysctl.conf`

### ulimit

`ulimit` 控制 shell 程序资源

``` sh
ulimit [-aHS] \
    [-c <core文件上限>] \
    [-d <数据节区大小>] \
    [-f <文件大小>] \
    [-m <内存大小>] \
    [-n <文件数目>] \
    [-p <缓冲区大小>] \
    [-s <堆栈大小>] \
    [-t <CPU时间>] \
    [-u <程序数目>] \
    [-v <虚拟内存大小>]
```

* -H 设置硬件资源限制,是管理员所设下的限制.
* -S 设置软件资源限制,是管理员所设下的限制.
* -a 显示当前所有的资源限制.
* -u 进程数目:用户最多可启动的进程数目.
* -c size:设置core文件的最大值.单位:blocks
* -d size:设置程序数据段的最大值.单位:kbytes
* -f size:设置shell创建文件的最大值.单位:blocks
* -l size:设置在内存中锁定进程的最大值.单位:kbytes
* -m size:设置可以使用的常驻内存的最大值.单位:kbytes
* -n size:设置内核可以同时打开的文件描述符的最大值.单位:n
* -p size:设置管道缓冲区的最大值.单位:kbytes
* -s size:设置堆栈的最大值.单位:kbytes
* -t size:设置CPU使用时间的最大上限.单位:seconds
* -v size:设置虚拟内存的最大值.单位:kbytes

内核可以同时打开的文件描述符默认是1024，在高负载下要设置为更高，可以通过以下命令修改

    $ ulimit -SHn 65535

ulimit只能做临时修改，重启后失效。

可以加入到 `/etc/rc.local` 文件，每次启动启用。

可以通过修改 `/etc/security/limits.conf` 永久调整 Linux 系统的最大进程数和最大文件打开数限制，

``` limits
#<domain>        <type>  <item>  <value>
* soft nproc 11000
* hard nproc 11000
* soft nofile 655350
* hard nofile 655350
```

同样需要注意 `/etc/security/limits.d` 目录下的文件，目录下文件与 `/etc/security/limits.conf` 有相同 domain 的配置时，会覆盖 `/etc/security/limits.conf` 中的配置

    cat /proc/29097/limits

    prlimit -n4096 -p pid_of_process

https://stackoverflow.com/questions/3734932/max-open-files-for-working-process

### oom

* `/proc/$PID/oom_adj`: 有效值是 -16 到 15，越大越容易被 kill。
* `/proc/$PID/oom_score`: 综合进程的内存消耗量、CPU时间(utime + stime)、存活时间(uptime - start time)和oom_adj计算出的，消耗内存越多分越高，存活时间越长分越低
* `/proc/$PID/oom_score_adj`: 有效值是 -1000 到 1000，Linux 2.6.36 之后用于替换 `/proc/$PID/oom_adj`

[](https://learning-kernel.readthedocs.io/en/latest/mem-management.html)
[](https://github.com/Yhzhtk/note/issues/31)
[](https://dev.to/rrampage/surviving-the-linux-oom-killer-2ki9)

### keepalive

`/etc/sysctl.conf`

``` sysctl
# 空闲时长/发送心跳的周期 7200s
net.ipv4.tcp_keepalive_time=7200
# 探测包发送间隔 75s
net.ipv4.tcp_keepalive_intvl=75
# 达到 tcp_keepalive_time 后，没有接收到对方确认，继续发送探测包次数
net.ipv4.tcp_keepalive_probes=9
```

### 磁盘使用率

    # df
    Filesystem      1K-blocks       Used Available Use% Mounted on
    /dev/vda1        20504188    4314736  15124852  23% /
    tmpfs            16321844          0  16321844   0% /dev/shm
    /dev/vdb       1548053708 1129305312 340088812  77% /data
    # node
    > 1129305312 / 1548053708
    0.7295000852774031
    > (1548053708 - 340088812) / 1548053708
    0.7803120071077018
    > 1548053708 - 1129305312 - 340088812
    78659584
    >

`df` 显示的 `Used + Available != 1K-blocks(total)`，这是因为 `ext2/3/4` 文件系统默认保留 `5%` 的的可用空间给 `root` 用户，这样即使磁盘使用率达到 100%，仍有系统组件和操作管理所需的工作空间

使用 `tune2fs -l /dev/vdb` 可用查看 Reserved block count 和其他文件系统的信息。

使用 `tune2fs -m <eserved-blocks-percentage> /dev/vdb` 调整保留块的百分比。

如果要计算使用率，可以使用 `(total - Available) / total`
