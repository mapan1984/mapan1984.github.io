## 组件

* Client：任务提交，生成 JobGraph
* Dispatcher：提供一个 REST 接口让 Client 提交作业，作业提交后，Dispatcher 会启动一个 JobManager 并将作业转交给它
* JobManager：调度 Job，通信，资源申请
* ResourceManager: 针对不同的环境（Yarn, K8s，独立部署），Flink 提供不同的 ResourceManager，负责管理可提供的 TaskManager 资源
* TaskManager：具体任务的执行，请求资源。TaskManager 启动后，会向 ResourceManager 注册它的 slot。TaskManager 在同一个 JVM 进程中以多线程的方式执行任务。

## 运行

通常情况下程序的 `main()` 方法会把 `Dataflow`/`JobGraph` 组装好，然后在 `StreamExecutionEnvironment.execute()` 方法被调用时，将其提交到远程的 JobManager 上。

本地执行模式：`execute()` 方法被调用时，会在同一个 JVM 中以独立线程的方式启动一个 JobManager 线程和一个 TaskManager 线程（默认的处理槽数等于 CPU 的核数），这样，整个 Flink 作业会以多线程的方式在同一个 JVM 进程中执行。

per-job 和 session 模式：`main()` 在 client 执行

application 模式：`main()` 在 jobmanager 执行

### StreamExecutionEnvironment

一般使用 `getExecutionEnvironment()` 静态方法获取执行环境，Flink 会根据代码调用时所处的上下文，返回一个本地或者远程环境

    val env = StreamExecutionEnvironment.getExecutionEnvironment

也可以显式地指定本地或者远程执行环境

    // 本地执行环境
    StreamExecutionEnvironment.createLocalEnvironment()

    // 远程执行环境
    StreamExecutionEnvironment.createRemoteEnvironment(
      "localhost",               // JobManager 的主机名
      1234,                      // JobManager 的端口号
      "/path/to/jarFile.jar"     // 需要传输到 JobManager 的 Jar 包（依赖，并不是程序本身）
    )

## 算子

1. 作用于单条记录的基本转换：map, filter, flatMap
    1. map: DataStream - DataStream
    2. filter: DataStream - DataStream
    3. flatMap
2. 针对相同键值事件的 KeyedStream 转换: keyBy
    1. keyBy: DataStream - KeyedStream
    2. 滚动聚合 reduce：KeyedStream - DataStream
        1. sum
        2. min
        3. max
        4. minBy
        5. maxBy
3. 合并多条数据流：union, connect, join
    1. union
    2. connect: DataStream.connect(DataStream) - ConnectedStreams
4. 拆分单条数据流：split
    1. split: DataStream.split(OutputSelector) - SplitStream; SplitStream.select() - DataStream
5. 基于窗口：window
    1. DataStream.keyBy() - KeyedStream.window() - WindowedStream.reduce/aggregate/process() - DataStream
    2. DataStream - AllWindowedStream - DataStream

## 配置

https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/config.html

`conf/flink-conf.yaml` 常用配置

### 资源

``` yaml
# The heap size for the JobManager JVM
jobmanager.heap.mb: 1024

# The heap size for the TaskManager JVM
taskmanager.heap.mb: 1024

# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.
taskmanager.numberOfTaskSlots: 4

# The managed memory size for each task manager.
taskmanager.managed.memory.size: 256
```

### checkpoint

``` yaml
# 任务使用的时间属性是 eventtime
pipeline.time-characteristic: EventTime

# checkpoint 的时间间隔
execution.checkpointing.interval: 10000
# 确保 checkpoint 之间的间隔
execution.checkpointing.min-pause: 60000
# 设置checkpoint的超时时间
execution.checkpointing.timeout: 60000
# 设置任务取消后保留hdfs上的checkpoint文件
execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION

state.backend: filesystem
state.checkpoints.dir: file:///tmp/flink/checkpoints
state.checkpoints.num-retained: 2
```

### 在命令中指定参数

``` bash
./bin/flink run -d -m yarn-cluster -t yarn-per-job \
    -yjm 1024 -ytm 1024 -ys 4 \
    -D yarn.containers.vcores=100 \
    jobs/FileSinkFromKafka.jar \
    -gid abcdefghj \
    -path hdfs://10.9.163.203:8020/flink-use/users
```

## 客户端操作接口

1. CommandLine
2. SQL Client
3. Scala Shell
4. Restful
5. Web

### CommandLine

#### standalone

帮助：

    $ flink -h
    $ flink run - h

运行：

    $ flink run ./examples/streaming/TopSpeedWindowing.jar

列出任务：

    $ flink list [-m 127.0.0.1:8081]

停止任务:

    $ flink stop <taskId>

取消任务：

    $ flink cancel <taskId> [-s /tmp/savepoint]

触发 savepoint：

    $ flink savepoint [-m 127.0.0.1:8081] <taskId> /tmp/savepoint

从指定 savepoint 启动：

    $ flink run -d -s /tmp/savepoint/savepoint-xxxxx ./examples/streaming/TopSpeedWindowing.jar

修改任务并行度：

    $ flink modify -p 3 <taskId>

任务信息：

    $ flink info ./examples/streaming/TopSpeedWindowing.jar

#### yarn per-job

    $ echo $HADOOP_CONF_DIR
    /etc/hadoop/conf/

attach 模式(客户端会一直等待直到程序结束才会退出)

    $ flink run -m yarn-cluster ./examples/batch/WordCount.jar

    $ flink run -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar

单任务detached

    $ flink run -yd -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar

#### yarn session

启动 session：

    $ yarn-session.sh -tm 2048 -s 3

在 session 中运行任务:

    $ flink run ./examples/batch/WordCount.jar

### Scala shell

## log

### 分类

JobManager 和 TaskManager 的日志可以在 `log` 子目录中找到：

* flink-${user}-standalonesession-${id}-${hostname}.log：代码中的日志输出
* flink-${user}-standalonesession-${id}-${hostname}.out：进程执行时的 stdout 输出
* flink-${user}-standalonesession-${id}-${hostname}-gc.log：JVM 的 GC 日志

* flink-${user}-taskexecutor-${id}-${hostname}.log：代码中的日志输出
* flink-${user}-taskexecutor-${id}-${hostname}.out：进程执行时的 stdout 输出
* flink-${user}-taskexecutor-${id}-${hostname}-gc.log：JVM 的 GC 日志

### 配置

* log4j-cli.properties：用 flink 命令行时用的 log 配置，比如执行 `flink run` 命令
* log4j-yarn-session.properties：用 `yarn-session.sh` 启动时命令行执行时用的 log 配置
* log4j.properties：无论是 standalone 还是 yarn 模式，JobManager 和 TaskManager 上的 log 配置都是 log4j.properties


## pid

    $ cat /tmp/flink-${user}.ssy-standalonesession.pid

    $ cat /tmp/flink-${user}.ssy-taskexecutor.pid

## 部署

### standalone

至少包含一个主进程和一个 TaskManager 进程，它们可以运行在一台或多台机器上，所有进程都作为普通的 JVM 进程来运行。

* 主进程 Master：以单独的线程来运行 Dispatcher 和 ResourceManager
* TaskManager 进程：将自身注册到主进程的 ResourceManager

提交作业流程：

1. Client 将作业提交到 Dispatcher
2. Dispatcher 在内部启动一个 JobManager
3. JobManager 向 ResourceManager 申请处理槽，并在处理槽准备完成后部署执行作业


#### 单机 standalone 集群

在主进程所在机器启动集群：

    $ ./bin/start-cluster.sh

    $ curl http://127.0.0.1:8081

在主进程所在机器停止集群：

    $ ./bin/stop-cluster.sh

#### 多机 standalone 集群

* 每台机器上配置好 Java 以及 JAVA_HOME 环境变量
* 挑选一台机器作为主进程机器，和其他机器 ssh 打通
* 每台机器上部署的 Flink binary 的目录要保证是同一个目录
* 如果需要用 hdfs，需要配置 HADOOP_CONF_DIR 环境变量

`conf/masters`

    hostname-1:8081

将所有需要运行 TaskManager 进程的机器名配置到 `conf/slaves`

    hostname-1
    hostname-2
    hostname-3

在所有机器上修改配置文件 `conf/flink-conf.yaml` 将 jobmanager.rpc.address 配置为主进程的地址

``` yaml
jobmanager.rpc.address: hostname-1
```

在主进程所在机器启动集群：

    $ ./bin/start-cluster.sh

    $ curl http://127.0.0.1:8081

在主进程所在机器停止集群：

    $ ./bin/stop-cluster.sh

### 多机 standalone 机器 HA

`conf/masters`

    hostname-1:8081
    hostname-2:8081


`conf/slaves`

    hostname-1
    hostname-2
    hostname-3


修改配置文件 `conf/flink-conf.yaml` 配置 flink HA 模式：

``` yaml
# 配置 high-availability mode
high-availability: zookeeper

# 配置 zookeeper quorum
high-availability.zookeeper.quorum: hostname:2181

# （可选）设置 zookeeper 的 root 目录
high-availability.zookeeper.path.root: /test_dir/test_standalone_root

# （可选）相当于是这个 standalone 集群中创建 zk node 的 namespace
high-availability.cluster-id: /test_dir/test_standalone


# JobManager 的 meta 信息放在 hdfs，在 zk 上主要会保存一个指向 hdfs 路径的指针
high-availability.storageDir: hdfs:///test_dir/recovery/
```

### Yarn 模式

* 资源按需使用，提高集群的资源利用率
* 任务有优先级，根据优先级运行作业
* 基于 Yarn 调度系统，能够自动化地处理各个角色的 failover
    * JobManager 进程和 TaskManager 进程都由 Yarn NodeManager 监控
    * 如果 JobManager 进程异常退出，则 Yarn ResourceManager 会重新调度 JobManager 到其他机器
    * 如果 TaskManager 进程异常退出，JobManager 会收到消息并重新向 Yarn ResourceManager 申请资源，重新启动 TaskManager


Flink 主进程作为 Yarn ApplicationMaster 启动

#### job mode

    $ ./bin/flink run -m yarn-cluster

#### session mode

Client 必须要设置 `YARN_CONF_DIR` 或者 `HADOOP_CONF_DIR` 环境变量，通过这个环境变量来读取 YARN 和 HDFS 的配置信息，否则启动会失败。

    $ ./bin/yarn-session.sh -h

    $ ./bin/yarn-session.sh -n 4 -jm 1024m -tm 4096m


`/tmp/.yarn-properties-${user}` 文件保存了上一次创建 yarn session 的集群信息

#### application mode

    $ export HADOOP_CLASSPATH=`hadoop classpath`

    $ export YARN_CONF_DIR=/conf/yarn

    $ export HADOOP_CONF_DIR=/hadoop/etc/hadoop

    $ bin/flink run-application -t yarn-application

    $ bin/flink run-application -t yarn-application  -Djobmanager.memory.process.size=2048m  -Dtaskmanager.memory.process.size=4096m  -Dyarn.application.name="MyFlinkWordCount"  ./examples/batch/WordCount.jar

#### HA

首先要确保启动 Yarn 集群用的 `yarn-site.xml` 文件中的这个配置，指定 Yarm 集群级别 AM 重启的上限：

``` xml
<property>
    <name>yarn.resourcemanager.am.max-attempts</name>
    <value>100</value>
</property>
```

然后在 `conf/flink-conf.yaml` 文件中配置这个 flink job 的 JobManager 能够重启的次数

``` yaml
yarn.application-attempts: 10
```

最后配置 zk 相关配置：

``` yaml
# 配置 high-availability mode
high-availability: zookeeper

# 配置 zookeeper quorum
high-availability.zookeeper.quorum: hostname:2181

# （可选）设置 zookeeper 的 root 目录
high-availability.zookeeper.path.root: /test_dir/test_standalone_root

# （可选）相当于是这个 standalone 集群中创建 zk node 的 namespace
# 使用 Yarn 的 application id，从而保证唯一性，否则，在提交作业的使用需要用户取保证 cluster-id 的全局唯一性
# high-availability.cluster-id: /test_dir/test_standalone


# JobManager 的 meta 信息放在 hdfs，在 zk 上主要会保存一个指向 hdfs 路径的指针
high-availability.storageDir: hdfs:///test_dir/recovery/
```

## 文件系统支持

### 添加新的外部文件系统实现

`org.apache.flink.core.fs.FileSystem`

### s3 文件系统实现

https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/s3.html

* flink-s3-fs-presto:
    * scheme: `s3://`, `s3p://`
    * checkpoint
* flink-s3-fs-hadoop:
    * scheme: `s3://`, `s3a://`
    * StreamingFileSink, FileSink

### 使用

#### s3

    mkdir ./plugins/s3-fs-hadoop
    cp ./opt/flink-s3-fs-hadoop-1.12.0.jar ./plugins/s3-fs-hadoop/

#### flink-conf.yaml


``` yaml
fs.s3a.endpoint: internal.s3-cn-bj.ufileos.com

fs.s3a.access.key: TOKEN_d601b138-69b7-426c-8555-35181f1981d2
fs.s3a.secret.key: 30bff6ed-5567-4dff-bdd7-c9d9687ef290

fs.s3a.connection.ssl.enabled: false
fs.s3a.path.style.access: true
fs.s3a.paging.maximum: 900
```

#### run

    ./bin/flink run -m yarn-cluster -yn 1 -yjm 1024 -ytm 1024  ./examples/batch/WordCount.jar -input s3a://kafka/init.vim -output s3a://kafka/wordcount-result.txt

#### File Sink

提供一个在流和批模式下统一的 Sink 来将分区文件写入到支持 Flink FileSystem 接口的文件系统中


#### 配置方式猜想

https://help.aliyun.com/document_detail/190663.html

##### 通过命令行设置 fs.s3a 参数

    flink run -m yarn-cluster -yD key1=value1 -yD key2=value2

``` bash
./bin/flink run -m yarn-cluster \
        -yn 1 -yjm 1024 -ytm 1024 \
        -yD fs.s3a.endpoint=internal.s3-cn-bj.ufileos.com \
        -yD fs.s3a.access.key=TOKEN_d601b138-69b7-426c-8555-35181f1981d2 \
        -yD fs.s3a.secret.key=30bff6ed-5567-4dff-bdd7-c9d9687ef290 \
        -yD fs.s3a.connection.ssl.enabled=false \
        -yD fs.s3a.path.style.access=true \
        -yD fs.s3a.paging.maximum=900 \
        ./examples/batch/WordCount.jar \
        -input s3a://kafka/init.vim \
        -output s3a://kafka/wordcout-result.txt
```
