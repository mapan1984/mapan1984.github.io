## 组件

* client：任务提交，生成 JobGraph
* JobManager：调度 Job，通信，资源申请
* TaskManager：具体任务的执行，请求资源

## 算子

1. 基于单条记录：filter, map
2. 基于窗口：window
3. 合并多条流：union, join, connect
4. 拆分单条流：split

## DataStream 基本转换

## 配置

https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/config.html

`conf/flink-conf.yaml` 常用配置

### 资源

``` yaml
# The heap size for the JobManager JVM
jobmanager.heap.mb: 1024

# The heap size for the TaskManager JVM
taskmanager.heap.mb: 1024

# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.
taskmanager.numberOfTaskSlots: 4

# The managed memory size for each task manager.
taskmanager.managed.memory.size: 256
```

### checkpoint

``` yaml
# 任务使用的时间属性是 eventtime
pipeline.time-characteristic: EventTime

# checkpoint 的时间间隔
execution.checkpointing.interval: 10000
# 确保 checkpoint 之间的间隔
execution.checkpointing.min-pause: 60000
# 设置checkpoint的超时时间
execution.checkpointing.timeout: 60000
# 设置任务取消后保留hdfs上的checkpoint文件
execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION

state.backend: filesystem
state.checkpoints.dir: file:///tmp/flink/checkpoints
state.checkpoints.num-retained: 2
```

### 在命令中指定参数

``` bash
./bin/flink run -d -m yarn-cluster -t yarn-per-job \
    -yjm 1024 -ytm 1024 -ys 4 \
    -D yarn.containers.vcores=100 \
    jobs/FileSinkFromKafka.jar \
    -gid abcdefghj \
    -path hdfs://10.9.163.203:8020/flink-use/users
```

## 客户端操作接口

1. CommandLine
2. SQL Client
3. Scala Shell
4. Restful
5. Web

### CommandLine

#### standalone

帮助：

    $ flink -h
    $ flink run - h

运行：

    $ flink run ./examples/streaming/TopSpeedWindowing.jar

列出任务：

    $ flink list [-m 127.0.0.1:8081]

停止任务:

    $ flink stop <taskId>

取消任务：

    $ flink cancel <taskId> [-s /tmp/savepoint]

触发 savepoint：

    $ flink savepoint [-m 127.0.0.1:8081] <taskId> /tmp/savepoint

从指定 savepoint 启动：

    $ flink run -d -s /tmp/savepoint/savepoint-xxxxx ./examples/streaming/TopSpeedWindowing.jar

修改任务并行度：

    $ flink modify -p 3 <taskId>

任务信息：

    $ flink info ./examples/streaming/TopSpeedWindowing.jar

#### yarn per-job

    $ echo $HADOOP_CONF_DIR
    /etc/hadoop/conf/

attach 模式(客户端会一直等待直到程序结束才会退出)

    $ flink run -m yarn-cluster ./examples/batch/WordCount.jar

    $ flink run -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar

单任务detached

    $ flink run -yd -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar

#### yarn session

启动 session：

    $ yarn-session.sh -tm 2048 -s 3

在 session 中运行任务:

    $ flink run ./examples/batch/WordCount.jar

### Scala shell

## log

### 分类

JobManager 和 TaskManager 的日志可以在 `log` 子目录中找到：

* flink-${user}-standalonesession-${id}-${hostname}.log：代码中的日志输出
* flink-${user}-standalonesession-${id}-${hostname}.out：进程执行时的 stdout 输出
* flink-${user}-standalonesession-${id}-${hostname}-gc.log：JVM 的 GC 日志

* flink-${user}-taskexecutor-${id}-${hostname}.log：代码中的日志输出
* flink-${user}-taskexecutor-${id}-${hostname}.out：进程执行时的 stdout 输出
* flink-${user}-taskexecutor-${id}-${hostname}-gc.log：JVM 的 GC 日志

### 配置

* log4j-cli.properties：用 flink 命令行时用的 log 配置，比如执行 `flink run` 命令
* log4j-yarn-session.properties：用 `yarn-session.sh` 启动时命令行执行时用的 log 配置
* log4j.properties：无论是 standalone 还是 yarn 模式，JobManager 和 TaskManager 上的 log 配置都是 log4j.properties


## pid

    $ cat /tmp/flink-${user}.ssy-standalonesession.pid

    $ cat /tmp/flink-${user}.ssy-taskexecutor.pid

## 部署

### standalone 集群

    $ ./bin/start-cluster.sh

    $ curl http://127.0.0.1:8081

    $ ./bin/stop-cluster.sh

### 多机 standalone 集群

* 每台机器上配置好 Java 以及 JAVA_HOME 环境变量
* 挑选一台机器，和其他机器 ssh 打通
* 每台机器上部署的 Flink binary 的目录要保证是同一个目录
* 如果需要用 hdfs，需要配置 HADOOP_CONF_DIR 环境变量

`conf/masters`

    hostname-1:8081


`conf/slaves`

    hostname-1
    hostname-2
    hostname-3

`conf/flink-conf.yaml`

``` yaml
jobmanager.rpc.address: hostname-1
```

### 多机 standalone 机器 HA

`conf/masters`

    hostname-1:8081
    hostname-2:8081


`conf/slaves`

    hostname-1
    hostname-2
    hostname-3


`conf/flink-conf.yaml`

``` yaml
# 配置 high-availability mode
high-availability: zookeeper

# 配置 zookeeper quorum
high-availability.zookeeper.quorum: hostname:2181

# （可选）设置 zookeeper 的 root 目录
high-availability.zookeeper.path.root: /test_dir/test_standalone_root

# （可选）相当于是这个 standalone 集群中创建 zk node 的 namespace
high-availability.cluster-id: /test_dir/test_standalone


# JobManager 的 meta 信息放在 hdfs，在 zk 上主要会保存一个指向 hdfs 路径的指针
high-availability.storageDir: hdfs:///test_dir/recovery/
```

### Yarn 模式

* 资源按需使用，提高集群的资源利用率
* 任务有优先级，根据优先级运行作业
* 基于 Yarn 调度系统，能够自动化地处理各个角色的 failover
    * JobManager 进程和 TaskManager 进程都由 Yarn NodeManager 监控
    * 如果 JobManager 进程异常退出，则 Yarn ResourceManager 会重新调度 JobManager 到其他机器
    * 如果 TaskManager 进程异常退出，JobManager 会收到消息并重新向 Yarn ResourceManager 申请资源，重新启动 TaskManager

#### long running

client必须要设置`YARN_CONF_DIR`或者`HADOOP_CONF_DIR`环境变量，通过这个环境变量来读取YARN和HDFS的配置信息，否则启动会失败。

    $ ./bin/yarn-session.sh -h

    $ ./bin/yarn-session.sh -n 4 -jm 1024m -tm 4096m


`/tmp/.yarn-properties-${user}` 文件保存了上一次创建 yarn session 的集群信息

#### HA

首先要确保启动 Yarn 集群用的 `yarn-site.xml` 文件中的这个配置，指定 Yarm 集群级别 AM 重启的上限：

``` xml
<property>
    <name>yarn.resourcemanager.am.max-attempts</name>
    <value>100</value>
</property>
```

然后在 `conf/flink-conf.yaml` 文件中配置这个 flink job 的 JobManager 能够重启的次数

``` yaml
yarn.application-attempts: 10
```

最后配置 zk 相关配置：

``` yaml
# 配置 high-availability mode
high-availability: zookeeper

# 配置 zookeeper quorum
high-availability.zookeeper.quorum: hostname:2181

# （可选）设置 zookeeper 的 root 目录
high-availability.zookeeper.path.root: /test_dir/test_standalone_root

# （可选）相当于是这个 standalone 集群中创建 zk node 的 namespace
# 使用 Yarn 的 application id，从而保证唯一性，否则，在提交作业的使用需要用户取保证 cluster-id 的全局唯一性
# high-availability.cluster-id: /test_dir/test_standalone


# JobManager 的 meta 信息放在 hdfs，在 zk 上主要会保存一个指向 hdfs 路径的指针
high-availability.storageDir: hdfs:///test_dir/recovery/
```

## s3 文件系统支持

### 添加新的外部文件系统实现

`org.apache.flink.core.fs.FileSystem`

### s3 文件系统实现

https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/s3.html

* flink-s3-fs-presto:
    * scheme: `s3://`, `s3p://`
    * checkpoint
* flink-s3-fs-hadoop:
    * scheme: `s3://`, `s3a://`
    * StreamingFileSink, FileSink

### 使用

#### s3

    mkdir ./plugins/s3-fs-hadoop
    cp ./opt/flink-s3-fs-hadoop-1.12.0.jar ./plugins/s3-fs-hadoop/

#### flink-conf.yaml


``` yaml
fs.s3a.endpoint: internal.s3-cn-bj.ufileos.com

fs.s3a.access.key: TOKEN_d601b138-69b7-426c-8555-35181f1981d2
fs.s3a.secret.key: 30bff6ed-5567-4dff-bdd7-c9d9687ef290

fs.s3a.connection.ssl.enabled: false
fs.s3a.path.style.access: true
fs.s3a.paging.maximum: 900
```

#### run

    ./bin/flink run -m yarn-cluster -yn 1 -yjm 1024 -ytm 1024  ./examples/batch/WordCount.jar -input s3a://kafka/init.vim -output s3a://kafka/wordcount-result.txt

#### File Sink

提供一个在流和批模式下统一的 Sink 来将分区文件写入到支持 Flink FileSystem 接口的文件系统中


#### 配置方式猜想

https://help.aliyun.com/document_detail/190663.html

##### 通过命令行设置 fs.s3a 参数

    flink run -m yarn-cluster -yD key1=value1 -yD key2=value2

``` bash
./bin/flink run -m yarn-cluster \
        -yn 1 -yjm 1024 -ytm 1024 \
        -yD fs.s3a.endpoint=internal.s3-cn-bj.ufileos.com \
        -yD fs.s3a.access.key=TOKEN_d601b138-69b7-426c-8555-35181f1981d2 \
        -yD fs.s3a.secret.key=30bff6ed-5567-4dff-bdd7-c9d9687ef290 \
        -yD fs.s3a.connection.ssl.enabled=false \
        -yD fs.s3a.path.style.access=true \
        -yD fs.s3a.paging.maximum=900 \
        ./examples/batch/WordCount.jar \
        -input s3a://kafka/init.vim \
        -output s3a://kafka/wordcout-result.txt
```
